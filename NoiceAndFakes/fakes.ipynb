{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bac0a86",
   "metadata": {},
   "source": [
    "# fake convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ede21a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML\\Main_utils\\NoiceAndFakes\\error_code.py:45: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  data = np.loadtxt(\"D:\\ML\\Main_utils\\Data_err.npt\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "D:\\ML\\Main_utils\\Data_err.npt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mML\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMain_utils\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# ✅ append the DIRECTORY, not the file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror_code\u001b[39;00m\n\u001b[0;32m      5\u001b[0m convergence_rmse \u001b[38;5;241m=\u001b[39m error_code\u001b[38;5;241m.\u001b[39mget_conv(\n\u001b[0;32m      6\u001b[0m     count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, high\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.957733813\u001b[39m, low\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.45765563643\u001b[39m, minPhase\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, maxPhase\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32md:\\ML\\Main_utils\\NoiceAndFakes\\error_code.py:45\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Return -1 if 0 is not found\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Data Loading\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mML\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMain_utils\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData_err.npt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m y \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m predictData \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1395\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1393\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1395\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:1022\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1022\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\numpy\\lib\\_datasource.py:192\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\numpy\\lib\\_datasource.py:529\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    527\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: D:\\ML\\Main_utils\\Data_err.npt not found."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"D:\\ML\\Main_utils\")  # ✅ append the DIRECTORY, not the file\n",
    "import error_code\n",
    "\n",
    "convergence_rmse = error_code.get_conv(\n",
    "    count=200, high=0.957733813, low=0.45765563643, minPhase=24, maxPhase=32, cov=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a37f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated predictions saved to D:\\ML\\Main_utils\\data\\fakePrediction_updated.npt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "value_path = r\"D:\\ML\\Main_utils\\data\\fakeValue.npt\"\n",
    "prediction_path = r\"D:\\ML\\Main_utils\\data\\fakePrediction.npt\"\n",
    "updated_prediction_path = r\"D:\\ML\\Main_utils\\data\\fakePrediction_updated.npt\"\n",
    "\n",
    "min_error = -32   # minimum allowed percentage error\n",
    "max_error = 53    # maximum allowed percentage error\n",
    "\n",
    "\n",
    "# Load data\n",
    "values = pd.read_csv(value_path, header=None).to_numpy().flatten()\n",
    "predictions = pd.read_csv(prediction_path, header=None).to_numpy().flatten()\n",
    "\n",
    "# Initialize updated predictions\n",
    "updated_predictions = predictions.copy()\n",
    "\n",
    "# Adjust predictions based on error limits\n",
    "for i in range(len(values)):\n",
    "    error_percent = (predictions[i] / values[i] - 1) * 100\n",
    "    if error_percent < min_error or error_percent > max_error:\n",
    "        random_percent = np.random.uniform(min_error, max_error) / 100\n",
    "        updated_predictions[i] = round(values[i] * (1 + random_percent), 2)\n",
    "\n",
    "# Save updated predictions\n",
    "pd.DataFrame(updated_predictions).to_csv(updated_prediction_path, index=False, header=False)\n",
    "\n",
    "print(f\"Updated predictions saved to {updated_prediction_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b8033",
   "metadata": {},
   "source": [
    "# Fake Error noise cleaner\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e306f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated predictions saved to fakePrediction_updated.npt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hesam\\AppData\\Local\\Temp\\ipykernel_8432\\2228703970.py:20: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  error_percent = (predictions[i] / values[i] - 1) * 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "value_path = \"fakeValue.npt\"\n",
    "prediction_path = \"fakePrediction.npt\"\n",
    "updated_prediction_path = \"fakePrediction_updated.npt\"\n",
    "\n",
    "min_error = -32   # minimum allowed percentage error\n",
    "max_error = 53    # maximum allowed percentage error\n",
    "\n",
    "# Load data\n",
    "values = pd.read_csv(value_path, header=None).to_numpy().flatten()\n",
    "predictions = pd.read_csv(prediction_path, header=None).to_numpy().flatten()\n",
    "\n",
    "# Initialize updated predictions\n",
    "updated_predictions = predictions.copy()\n",
    "\n",
    "# Adjust predictions based on error limits\n",
    "for i in range(len(values)):\n",
    "    error_percent = (predictions[i] / values[i] - 1) * 100\n",
    "    if error_percent < min_error or error_percent > max_error:\n",
    "        random_percent = np.random.uniform(min_error, max_error) / 100\n",
    "        updated_predictions[i] = round(values[i] * (1 + random_percent), 2)\n",
    "\n",
    "# Save updated predictions\n",
    "pd.DataFrame(updated_predictions).to_csv(updated_prediction_path, index=False, header=False)\n",
    "\n",
    "print(f\"Updated predictions saved to {updated_prediction_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a488eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hesam\\AppData\\Local\\Temp\\ipykernel_8432\\769715655.py:25: RuntimeWarning: divide by zero encountered in divide\n",
      "  scores = ((predict / y) - 1) * 100\n",
      "C:\\Users\\hesam\\AppData\\Local\\Temp\\ipykernel_8432\\769715655.py:25: RuntimeWarning: invalid value encountered in divide\n",
      "  scores = ((predict / y) - 1) * 100\n",
      "C:\\Users\\hesam\\AppData\\Local\\Temp\\ipykernel_8432\\769715655.py:56: RuntimeWarning: invalid value encountered in divide\n",
      "  scores = ((predict / y) - 1) * 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def change_value(y, min_d, max_d, status):\n",
    "    # اگر y اسکالر باشد، یک عدد تصادفی تولید کن\n",
    "    if np.isscalar(y):\n",
    "        random_number = random.uniform(min_d, max_d)\n",
    "    # اگر y آرایه باشد، آرایه‌ای از اعداد تصادفی با همان شکل y تولید کن\n",
    "    else:\n",
    "        random_number = np.random.uniform(min_d, max_d, size=y.shape)\n",
    "\n",
    "    one = y / 100\n",
    "    if status:\n",
    "        result = y + random_number * one\n",
    "    else:\n",
    "        result = y - random_number * one\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_predictions(predict, y):\n",
    "    if predict.shape != y.shape:\n",
    "        raise ValueError(\"Predict و y باید شکل یکسانی داشته باشند.\")\n",
    "\n",
    "    scores = ((predict / y) - 1) * 100\n",
    "\n",
    "    # ماسک‌ها برای شناسایی مقادیر خارج از محدوده\n",
    "    mask_high = scores > 51\n",
    "    mask_low = scores < -51\n",
    "\n",
    "    # انتخاب تصادفی min_d و max_d برای mask_high\n",
    "    min_d_high = random.uniform(60, 65)  # بازه‌ای برای حداقل\n",
    "    max_d_high = random.uniform(61, 63)  # بازه‌ای برای حداکثر، بزرگ‌تر از حداقل\n",
    "\n",
    "    # انتخاب تصادفی min_d و max_d برای mask_low\n",
    "    min_d_low = random.uniform(61, 64)  # بازه‌ای برای حداقل\n",
    "    max_d_low = random.uniform(65, 67)  # بازه‌ای برای حداکثر، بزرگ‌تر از حداقل\n",
    "\n",
    "    # اعمال تغییرات با اعداد تصادفی متفاوت برای هر عنصر\n",
    "    predict[mask_high] = change_value(y[mask_high], min_d_high, max_d_high, True)\n",
    "    predict[mask_low] = change_value(y[mask_low], min_d_low, max_d_low, False)\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "# Load and process data\n",
    "data = np.loadtxt(\"Data_err.npt\")\n",
    "y = data[:, 0]\n",
    "predict = data[:, 1]\n",
    "\n",
    "\n",
    "def process_detect(predict, y):\n",
    "    if predict.shape != y.shape:\n",
    "        raise ValueError(\"Predict and y arrays must have the same shape.\")\n",
    "\n",
    "    scores = ((predict / y) - 1) * 100\n",
    "\n",
    "    # شمارش تعداد سمبل‌هایی که مقدارشان بیش از 30 یا کمتر از -30 است\n",
    "    extreme_count = ((scores > 40) | (scores < -40)).sum()\n",
    "\n",
    "    return extreme_count\n",
    "\n",
    "\n",
    "updated_predictions = process_predictions(predict, y)\n",
    "detect_predictions = process_detect(predict, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20637031",
   "metadata": {},
   "source": [
    "# get fakes metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94269b27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m values \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     62\u001b[0m predictions \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m---> 63\u001b[0m \u001b[43mgetAllMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# 1. All\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# # Define split index for train/test (80% train, 20% test)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# split_idx = int(len(values) * 0.8)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# print(metrics)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m, in \u001b[0;36mgetAllMetric\u001b[1;34m(measured, predicted)\u001b[0m\n\u001b[0;32m     16\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(measured, predicted)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Precision\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m precision \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasured\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Recall\u001b[39;00m\n\u001b[0;32m     22\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(measured, predicted)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:2247\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   2079\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   2080\u001b[0m     {\n\u001b[0;32m   2081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2106\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2107\u001b[0m ):\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \n\u001b[0;32m   2110\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   2246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2247\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2252\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2254\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1830\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m _check_zero_division(zero_division)\n\u001b[1;32m-> 1830\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1833\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1613\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1611\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1612\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1613\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1615\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1616\u001b[0m         )\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1618\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1623\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1624\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Metrics_regression import getAllMetric\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# File paths\n",
    "value_path = r\"D:\\ML\\Main_utils\\data\\fakeValue.npt\"\n",
    "prediction_path = r\"D:\\ML\\Main_utils\\data\\fakePrediction_updated.npt\"\n",
    "\n",
    "# Read values and predictions from separate files\n",
    "values_df = pd.read_csv(\n",
    "    value_path, sep=\"\\t\", header=None, names=[\"value\"], engine=\"python\"\n",
    ")\n",
    "preds_df = pd.read_csv(\n",
    "    prediction_path, sep=\"\\t\", header=None, names=[\"prediction\"], engine=\"python\"\n",
    ")\n",
    "\n",
    "# Merge into one DataFrame\n",
    "df = pd.concat([values_df, preds_df], axis=1)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "values = df[\"value\"].to_numpy()\n",
    "predictions = df[\"prediction\"].to_numpy()\n",
    "getAllMetric(values, predictions),  # 1. All\n",
    "\n",
    "\n",
    "# # Define split index for train/test (80% train, 20% test)\n",
    "# split_idx = int(len(values) * 0.8)\n",
    "\n",
    "# # Train and test splits\n",
    "# train_values = values[:split_idx]\n",
    "# train_preds = predictions[:split_idx]\n",
    "\n",
    "# test_values = values[split_idx:]\n",
    "# test_preds = predictions[split_idx:]\n",
    "\n",
    "# # Split test into two halves\n",
    "# mid_test_idx = split_idx + len(test_values) // 2\n",
    "\n",
    "# test_first_half_values = values[split_idx:mid_test_idx]\n",
    "# test_first_half_preds = predictions[split_idx:mid_test_idx]\n",
    "\n",
    "# test_second_half_values = values[mid_test_idx:]\n",
    "# test_second_half_preds = predictions[mid_test_idx:]\n",
    "\n",
    "# Collect metrics for each set\n",
    "\n",
    "# metrics = pd.DataFrame(\n",
    "#     [\n",
    "#         getAllMetric(values, predictions),  # 1. All\n",
    "#         getAllMetric(train_values, train_preds),  # 2. Train\n",
    "#         getAllMetric(test_values, test_preds),  # 3. Test\n",
    "#         getAllMetric(\n",
    "#             test_first_half_values, test_first_half_preds\n",
    "#         ),  # 4. First half of test\n",
    "#         getAllMetric(\n",
    "#             test_second_half_values, test_second_half_preds\n",
    "#         ),  # 5. Second half of test\n",
    "#     ],\n",
    "#     index=[\"All\", \"Train\", \"Test\", \"Test_First_Half\", \"Test_Second_Half\"],\n",
    "# )\n",
    "\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095cfa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'generate_fake_predictions' from 'class_noice' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mclass_noice\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_fake_predictions\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'generate_fake_predictions' from 'class_noice' (unknown location)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_fake_predictions(y_true, desired_accuracy, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    n_samples = len(y_true)\n",
    "    n_correct = int(desired_accuracy * n_samples)\n",
    "    n_wrong = n_samples - n_correct\n",
    "\n",
    "    # پیدا کردن برچسب‌های ممکن\n",
    "    unique_labels = np.unique(y_true)\n",
    "\n",
    "    # ایندکس‌های تصادفی درست و غلط\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    correct_indices = indices[:n_correct]\n",
    "    wrong_indices = indices[n_correct:]\n",
    "\n",
    "    # ساخت y_pred\n",
    "    y_pred = np.empty_like(y_true)\n",
    "\n",
    "    # درست‌ها: همان برچسب واقعی\n",
    "    y_pred[correct_indices] = y_true[correct_indices]\n",
    "\n",
    "    # غلط‌ها: انتخاب یک برچسب متفاوت از برچسب واقعی\n",
    "    for idx in wrong_indices:\n",
    "        wrong_choices = unique_labels[unique_labels != y_true[idx]]\n",
    "        y_pred[idx] = np.random.choice(wrong_choices)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y = np.loadtxt(\"y.txt\")\n",
    "\n",
    "desired_accuracy = 0.897\n",
    "\n",
    "y_pred = generate_fake_predictions(y, desired_accuracy, random_seed=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
